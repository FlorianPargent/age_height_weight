@article{maulRethinking2017,
author = {Andrew Maul},
title = {Rethinking Traditional Methods of Survey Validation},
journal = {Measurement: Interdisciplinary Research and Perspectives},
volume = {15},
number = {2},
pages = {51--69},
year = {2017},
publisher = {Routledge},
doi = {10.1080/15366367.2017.1348108},
URL = {https://doi.org/10.1080/15366367.2017.1348108},
eprint = {https://doi.org/10.1080/15366367.2017.1348108}
}

@thesis{eichhornItem2019,
           title = {Item und Skala: Empirische Untersuchungen zur G{\"u}ltigkeit psychologischer Messungen anhand physikalischer Merkmale},
       publisher = {Ludwig-Maximilians-Universit{\"a}t M{\"u}nchen},
          author = {Kathryn Eichhorn},
            year = {2019},
           month = {Februar},
             url = {http://nbn-resolving.de/urn:nbn:de:bvb:19-236286}}

@software{schneiderApaquarto2025, 
  author = {Schneider, William Joel},
  license = {CC0-1.0},
  title = {{apaquarto}},
  year = {2025},
  url = {https://github.com/wjschne/apaquarto}}

@Manual{positRstudio2025,
    title = {RStudio: Integrated Development Environment for R},
    author = {{Posit team}},
    organization = {Posit Software, PBC},
    address = {Boston, MA},
    year = {2025},
    url = {http://www.posit.co/},
  }

@software{allaireQuarto2025,
author = {Allaire, J.J. and Teague, Charles and Scheidegger, Carlos and Xie, Yihui and Dervieux, Christophe and Woodhull, Gordon},
doi = {10.5281/zenodo.5960048},
month = apr,
title = {{Quarto}},
url = {https://github.com/quarto-dev/quarto-cli},
version = {1.7},
year = {2025}
}

@article{bortolottiRelevanceAdvantagesUsing2013,
  title = {Relevance and Advantages of Using the Item Response Theory},
  author = {Bortolotti, Silvana Ligia Vincenzi and Tezza, Rafael and De Andrade, Dalton Francisco and Bornia, Antonio Cezar and De Sousa J{\'u}nior, Afonso Farias},
  year = {2013},
  month = jun,
  journal = {Quality \& Quantity},
  volume = {47},
  number = {4},
  pages = {2341--2360},
  issn = {0033-5177, 1573-7845},
  doi = {10.1007/s11135-012-9684-5},
  urldate = {2025-06-03},
  copyright = {http://www.springer.com/tdm},
  langid = {english}
}

@article{bradleyRatingScalesSurvey2015,
  title = {Rating {{Scales}} in {{Survey Research}}: {{Using}} the {{Rasch}} Model to Illustrate the Middle Category Measurement Flaw},
  shorttitle = {Rating {{Scales}} in {{Survey Research}}},
  author = {Bradley, Kelly D. and Peabody, Michael R. and Akers, Kathryn S. and Knutson, Nichole M.},
  year = {2015},
  month = may,
  journal = {Survey Practice},
  volume = {8},
  number = {1},
  pages = {1--12},
  issn = {2168-0094},
  doi = {10.29115/SP-2015-0001},
  urldate = {2025-06-03}
}

@article{broughtonPrototypeStrategyConstruction1984,
  title = {A Prototype Strategy for Construction of Personality Scales.},
  author = {Broughton, Ross},
  year = {1984},
  month = dec,
  journal = {Journal of Personality and Social Psychology},
  volume = {47},
  number = {6},
  pages = {1334--1346},
  issn = {1939-1315, 0022-3514},
  doi = {10.1037/0022-3514.47.6.1334},
  urldate = {2025-06-03},
  langid = {english}
}

@article{burischApproachesPersonalityInventory1984,
  title = {Approaches to Personality Inventory Construction: {{A}} Comparison of Merits.},
  shorttitle = {Approaches to Personality Inventory Construction},
  author = {Burisch, Matthias},
  year = {1984},
  month = mar,
  journal = {American Psychologist},
  volume = {39},
  number = {3},
  pages = {214--227},
  issn = {1935-990X, 0003-066X},
  doi = {10.1037/0003-066X.39.3.214},
  urldate = {2025-06-03},
  langid = {english}
}

@article{debelakTestingLocalIndependence2020,
  title = {Testing the {{Local Independence Assumption}} of the {{Rasch Model With}} {{{\emph{Q}}}}{\textsubscript{3}} -{{Based Nonparametric Model Tests}}},
  author = {Debelak, Rudolf and Koller, Ingrid},
  year = {2020},
  month = mar,
  journal = {Applied Psychological Measurement},
  volume = {44},
  number = {2},
  pages = {103--117},
  issn = {0146-6216, 1552-3497},
  doi = {10.1177/0146621619835501},
  urldate = {2025-06-03},
  abstract = {Local independence is a central assumption of commonly used item response theory models. Violations of this assumption are usually tested using test statistics based on item pairs. This study presents two quasi-exact tests based on the [Formula: see text] statistic for testing the hypothesis of local independence in the Rasch model. The proposed tests do not require the estimation of item parameters and can also be applied to small data sets. The authors evaluate the tests with three simulation studies. Their results indicate that the quasi-exact tests hold their alpha level under the Rasch model and have higher power against different forms of local dependence than several alternative parametric and nonparametric model tests for local independence.},
  langid = {english}
}

@book{faulbaumWasIstGute2009,
  title = {{Was ist eine gute Frage?}},
  author = {Faulbaum, Frank and Pr{\"u}fer, Peter and Rexroth, Margrit},
  year = {2009},
  publisher = {VS Verlag f{\"u}r Sozialwissenschaften},
  address = {Wiesbaden},
  doi = {10.1007/978-3-531-91441-1},
  urldate = {2025-06-03},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-531-15824-2 978-3-531-91441-1},
  langid = {ngerman},
  file = {/Users/elinaschweppe/Zotero/storage/MLS5BSE4/Faulbaum et al. - 2009 - Was ist eine gute Frage.pdf}
}

@article{goretzkoEvaluatingModelFit2024,
  title = {Evaluating {{Model Fit}} of {{Measurement Models}} in {{Confirmatory Factor Analysis}}},
  author = {Goretzko, David and Siemund, Karik and Sterner, Philipp},
  year = {2024},
  month = feb,
  journal = {Educational and Psychological Measurement},
  volume = {84},
  number = {1},
  pages = {123--144},
  issn = {0013-1644, 1552-3888},
  doi = {10.1177/00131644231163813},
  urldate = {2025-06-03},
  abstract = {Confirmatory factor analyses (CFA) are often used in psychological research when developing measurement models for psychological constructs. Evaluating CFA model fit can be quite challenging, as tests for exact model fit may focus on negligible deviances, while fit indices cannot be interpreted absolutely without specifying thresholds or cutoffs. In this study, we review how model fit in CFA is evaluated in psychological research using fit indices and compare the reported values with established cutoff rules. For this, we collected data on all CFA models in Psychological Assessment from the years 2015 to 2020 [Formula: see text]. In addition, we reevaluate model fit with newly developed methods that derive fit index cutoffs that are tailored to the respective measurement model and the data characteristics at hand. The results of our review indicate that the model fit in many studies has to be seen critically, especially with regard to the usually imposed independent clusters constraints. In addition, many studies do not fully report all results that are necessary to re-evaluate model fit. We discuss these findings against new developments in model fit evaluation and methods for specification search.},
  langid = {english},
  file = {/Users/elinaschweppe/Zotero/storage/922YG2TA/Goretzko et al. - 2024 - Evaluating Model Fit of Measurement Models in Conf.pdf}
}

@article{hilbertInfluenceResponseFormat2016,
  title = {The Influence of the Response Format in a Personality Questionnaire: {{An}} Analysis of a Dichotomous, a {{Likert-type}}, and a Visual Analogue Scale},
  shorttitle = {The Influence of the Response Format in a Personality Questionnaire},
  author = {Hilbert, Sven and Küchenhoff, Helmut and Sarubin, Nina and Toyo Nakagawa, Tristan and Bühner, Markus},
  year = {2016},
  journal = {TPM - Testing, Psychometrics, Methodology in Applied Psychology},
  volume = {23},
  number = {1},
  pages = {3--24},
  issn = {1972-6325},
  doi = {10.4473/TPM23.1.1},
  urldate = {2025-06-03},
  abstract = {In the present study, 866 participants completed a questionnaire on the personality facet ``dutifulness.'' A dichotomous (DS), a 5-point Likert-type (LTS), and a 100-mm visual analogue scale (VAS) were analyzed regarding their effects on psychometric properties in a repeated measures design. Concerning estimates of reliability, it was shown that Cronbach's alpha increased with the number of response alternatives of the scales, while McDonald's omega and validity coefficients remained steady. It is argued that, given the {$\tau$}-congeneric measurement model of the present study, omega provides the more adequate estimate of internal consistency. Generalized linear regression analyses indicated that the participants judged the intervals between varying levels of dutifulness on the VAS differently from the other two scales. It is concluded that the choice of response format should not be exclusively based on desired psychometric properties but rather on practical considerations.},
  langid = {english}
}

@article{hilbertWhatMeasureEmpirical2022,
  title = {What's the Measure? {{An}} Empirical Investigation of Self-Ratings on Response Scales},
  shorttitle = {What's the Measure?},
  author = {Hilbert, Sven and Pargent, Florian and Kraus, Elisabeth and Naumann, Felix and Eichhorn, Kathryn and Ungar, Patrizia and B{\"u}hner, Markus},
  year = {2022},
  month = jan,
  journal = {International Journal of Social Research Methodology},
  volume = {25},
  number = {1},
  pages = {59--78},
  issn = {1364-5579, 1464-5300},
  doi = {10.1080/13645579.2020.1839163},
  urldate = {2025-06-03},
  langid = {english},
  file = {/Users/elinaschweppe/Zotero/storage/VPZEYP7F/Hilbert et al. - 2022 - What’s the measure An empirical investigation of .pdf}
}

@misc{kelleyMBESSMBESSPackage2023,
  title = {{{MBESS}}: {{The MBESS R Package}}},
  shorttitle = {{{MBESS}}},
  author = {Kelley, Ken},
  year = {2023},
  month = oct,
  pages = {4.9.3},
  publisher = {Comprehensive R Archive Network},
  doi = {10.32614/CRAN.package.MBESS},
  urldate = {2025-06-03},
  abstract = {Implements methods that are useful in designing research studies and analyzing data, with  particular emphasis on methods that are developed for or used within the behavioral,  educational, and social sciences (broadly defined). That being said, many of the methods  implemented within MBESS are applicable to a wide variety of disciplines. MBESS has a  suite of functions for a variety of related topics, such as effect sizes, confidence intervals  for effect sizes (including standardized effect sizes and noncentral effect sizes), sample size planning (from the accuracy in parameter estimation [AIPE], power analytic, equivalence, and  minimum-risk point estimation perspectives), mediation analysis, various properties of  distributions, and a variety of utility functions. MBESS (pronounced 'em-bes') was originally  an acronym for 'Methods for the Behavioral, Educational, and Social Sciences,' but MBESS became more general and now contains methods applicable and used in a wide variety of fields and is an  orphan acronym, in the sense that what was an acronym is now literally its name. MBESS has  greatly benefited from others, see {$<$}https://www3.nd.edu/{\textasciitilde}kkelley/site/MBESS.html{$>$} for a detailed  list of those that have contributed and other details.},
  langid = {english}
}

@article{leeSearchOptimalNumber2014,
  title = {In {{Search}} of the {{Optimal Number}} of {{Response Categories}} in a {{Rating Scale}}},
  author = {Lee, Jihyun and Paek, Insu},
  year = {2014},
  month = oct,
  journal = {Journal of Psychoeducational Assessment},
  volume = {32},
  number = {7},
  pages = {663--673},
  issn = {0734-2829, 1557-5144},
  doi = {10.1177/0734282914522200},
  urldate = {2025-06-03},
  abstract = {Likert-type rating scales are still the most widely used method when measuring psychoeducational constructs. The present study investigates a long-standing issue of identifying the optimal number of response categories. A special emphasis is given to categorical data, which were generated by the Item Response Theory (IRT) Graded-Response Modeling (GRM). Along with number of categories (from 2 to 6), two scale characteristics of scale length ( n = 5, 10, and 20 items) and item discrimination (high/medium/low) were examined. Results of this study show that there was virtually no difference in psychometric properties of the scales using 4, 5, or 6 categories. Most deteriorating change was observed when the number of response categories reduced from 3 to 2 points in all six psychometric measures. Small moderating effects by scale length and item discrimination seem to be present, that is, a slightly larger impact on the psychometric properties by changing the number of response categories in a shorter and/or highly discriminating scale. This study concludes with the suggestion that a caution should be made if a scale has only 2 response categories but that limitation may be overcome by manipulating other scale features, namely, scale length or item discrimination.},
  langid = {english}
}

@book{lordStatisticalTheoriesMental1968,
  title = {Statistical Theories of Mental Test Scores},
  author = {Lord, Frederic M. and Novick, Melvin R. and Birnbaum, Allan and Lord, Frederic M.},
  year = {1968},
  series = {Addison-{{Wesley}} Series in Behavioral Science Quantitative Methods},
  edition = {2. print},
  number = {4310},
  publisher = {Addison-Wesley},
  address = {Reading, Mass.},
  isbn = {978-0-201-04310-5},
  langid = {english},
  file = {/Users/elinaschweppe/Zotero/storage/A6IR83ZM/Lord et al. - 1974 - Statistical theories of mental test scores.pdf}
}

@book{martinAnthropometrieAnleitungSelbstaendigen1929,
  title = {{Anthropometrie: Anleitung Zu Selbst{\"a}ndigen Anthropologischen Erhebungen}},
  shorttitle = {{Anthropometrie}},
  author = {Martin, Rudolf},
  year = {1929},
  edition = {2nd ed},
  publisher = {Springer Berlin / Heidelberg},
  address = {Berlin, Heidelberg},
  isbn = {978-3-662-31474-6 978-3-662-31681-8},
  langid = {german}
}

@article{masudaRespondentsLowMotivation2017,
  title = {Respondents with Low Motivation Tend to Choose Middle Category: Survey Questions on Happiness in {{Japan}}},
  shorttitle = {Respondents with Low Motivation Tend to Choose Middle Category},
  author = {Masuda, Shinya and Sakagami, Takayuki and Kawabata, Hideaki and Kijima, Nobuhiko and Hoshino, Takahiro},
  year = {2017},
  month = jul,
  journal = {Behaviormetrika},
  volume = {44},
  number = {2},
  pages = {593--605},
  issn = {0385-7417, 1349-6964},
  doi = {10.1007/s41237-017-0026-8},
  urldate = {2025-06-03},
  langid = {english}
}

@article{mccrearyGenderAgeDifferences2002,
  title = {Gender and {{Age Differences}} in the {{Relationship Between Body Mass Index}} and {{Perceived Weight}}: {{Exploring}} the {{Paradox}}},
  shorttitle = {Gender and {{Age Differences}} in the {{Relationship Between Body Mass Index}} and {{Perceived Weight}}},
  author = {McCreary, Donald},
  year = {2002},
  month = jan,
  journal = {International Journal of Men's Health},
  volume = {1},
  number = {1},
  pages = {31--42},
  issn = {1532-6306, 1933-0278},
  doi = {10.3149/jmh.0101.31},
  urldate = {2025-06-03}
}

@article{mcneishThornyRelationMeasurement2018,
  title = {The {{Thorny Relation Between Measurement Quality}} and {{Fit Index Cutoffs}} in {{Latent Variable Models}}},
  author = {McNeish, Daniel and An, Ji and Hancock, Gregory R.},
  year = {2018},
  month = jan,
  journal = {Journal of Personality Assessment},
  volume = {100},
  number = {1},
  pages = {43--52},
  issn = {0022-3891, 1532-7752},
  doi = {10.1080/00223891.2017.1281286},
  urldate = {2025-06-03},
  langid = {english}
}

@article{montepareYouReOnly1989,
  title = {"{{You}}'re Only as Old as You Feel": {{Self-perceptions}} of Age, Fears of Aging, and Life Satisfaction from Adolescence to Old Age.},
  shorttitle = {"{{You}}'re Only as Old as You Feel"},
  author = {Montepare, Joann M. and Lachman, Margie E.},
  year = {1989},
  month = mar,
  journal = {Psychology and Aging},
  volume = {4},
  number = {1},
  pages = {73--78},
  issn = {1939-1498, 0882-7974},
  doi = {10.1037/0882-7974.4.1.73},
  urldate = {2025-06-03},
  langid = {english}
}

@article{pargentCanMakeIt2019,
  title = {Can't {{Make}} It {{Better}} nor {{Worse}}: {{An Empirical Study About}} the {{Effectiveness}} of {{General Rules}} of {{Item Construction}} on {{Psychometric Properties}}},
  shorttitle = {Can't {{Make}} It {{Better}} nor {{Worse}}},
  author = {Pargent, Florian and Hilbert, Sven and Eichhorn, Kathryn and B{\"u}hner, Markus},
  year = {2019},
  month = nov,
  journal = {European Journal of Psychological Assessment},
  volume = {35},
  number = {6},
  pages = {891--899},
  issn = {1015-5759, 2151-2426},
  doi = {10.1027/1015-5759/a000471},
  urldate = {2025-06-03},
  abstract = {Abstract. Some of the most popular psychological questionnaires violate general rules of item construction: precise, positively keyed items without negations, multiple aspects of content, absolute statements, or vague quantifiers. To investigate if following these rules results in more desirable psychometric properties, 1,733 participants completed online either the original NEO Five-Factor Inventory, an ``improved'' version whose items follow the rules of item construction, or a ``deteriorated'' version whose items strongly violate these rules. We compared reliability estimates, item-total correlations, Confirmatory Factor Analysis (CFA) model fit, and fit to the partial credit model between the three versions. Neither of the manipulations resulted in considerable or consistent effects on any of the psychometric indices. Our results question the ability of standard analyses in test construction to distinguish good items from bad ones, as well as the effectiveness of general rules of item construction. To increase the reproducibility of psychological science, more focus should be laid on improving psychological measures.},
  langid = {english},
  file = {/Users/elinaschweppe/Zotero/storage/DUX7G2TV/Pargent et al. - 2019 - Can’t Make it Better nor Worse An Empirical Study.pdf}
}

@article{rosseelLavaanPackageStructural2012,
  title = {{\textbf{Lavaan}} : {{An}} {{{\emph{R}}}} {{Package}} for {{Structural Equation Modeling}}},
  shorttitle = {{\textbf{Lavaan}}},
  author = {Rosseel, Yves},
  year = {2012},
  journal = {Journal of Statistical Software},
  volume = {48},
  number = {2},
  issn = {1548-7660},
  doi = {10.18637/jss.v048.i02},
  urldate = {2025-06-03},
  langid = {english}
}

@article{prestonOptimalNumberResponse2000,
  title = {Optimal Number of Response Categories in Rating Scales: Reliability, Validity, Discriminating Power, and Respondent Preferences},
  shorttitle = {Optimal Number of Response Categories in Rating Scales},
  author = {Preston, Carolyn C and Colman, Andrew M},
  year = {2000},
  month = mar,
  journal = {Acta Psychologica},
  volume = {104},
  number = {1},
  pages = {1--15},
  issn = {00016918},
  doi = {10.1016/S0001-6918(99)00050-5},
  urldate = {2025-06-03},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/elinaschweppe/Zotero/storage/M6PBTSIM/Preston und Colman - 2000 - Optimal number of response categories in rating sc.pdf}
}

@article{revillaChoosingNumberCategories2014,
  title = {Choosing the {{Number}} of {{Categories}} in {{Agree}}--{{Disagree Scales}}},
  author = {Revilla, Melanie A. and Saris, Willem E. and Krosnick, Jon A.},
  year = {2014},
  month = feb,
  journal = {Sociological Methods \& Research},
  volume = {43},
  number = {1},
  pages = {73--97},
  issn = {0049-1241, 1552-8294},
  doi = {10.1177/0049124113509605},
  urldate = {2025-06-03},
  abstract = {Although agree--disagree (AD) rating scales suffer from acquiescence response bias, entail enhanced cognitive burden, and yield data of lower quality, these scales remain popular with researchers due to practical considerations (e.g., ease of item preparation, speed of administration, and reduced administration costs). This article shows that if researchers want to use AD scales, they should offer 5 answer categories rather than 7 or 11, because the latter yield data of lower quality. This is shown using data from four multitrait-multimethod experiments implemented in the third round of the European Social Survey. The quality of items with different rating scale lengths were computed and compared.},
  langid = {english}
}

@book{schwallDefiningAgeUsing2012,
  title = {Defining {{Age}} and {{Using Age-Relevant Constructs}}},
  author = {Schwall, Alexander R.},
  year = {2012},
  month = mar,
  publisher = {Oxford University Press},
  doi = {10.1093/oxfordhb/9780195385052.013.0080},
  urldate = {2025-06-03}
}

@incollection{vanderlindenUnidimensionalLogisticResponse2016,
  author       = {{Van der Linden}, Wim J.},
  title        = {Unidimensional Logistic Response Models},
  editor       = {{Van der Linden}, Wim J.},
  booktitle    = {Handbook of Item Response Theory, Volume One: Models},
  series       = {Chapman \& Hall/CRC Statistics in the Social and Behavioral Sciences},
  publisher    = {Chapman \& Hall/CRC},
  address      = {Boca Raton, FL},
  year         = {2016},
  pages        = {13--30},
  doi          = {10.1201/9781315374512},
}

@article{weijtersEffectRatingScale2010,
  title = {The Effect of Rating Scale Format on Response Styles: {{The}} Number of Response Categories and Response Category Labels},
  shorttitle = {The Effect of Rating Scale Format on Response Styles},
  author = {Weijters, Bert and Cabooter, Elke and Schillewaert, Niels},
  year = {2010},
  month = sep,
  journal = {International Journal of Research in Marketing},
  volume = {27},
  number = {3},
  pages = {236--247},
  issn = {01678116},
  doi = {10.1016/j.ijresmar.2010.02.004},
  urldate = {2025-06-03},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@book{willisCognitiveInterviewing2005,
  title = {Cognitive {{Interviewing}}},
  author = {Willis, Gordon},
  year = {2005},
  publisher = {SAGE Publications, Inc.},
  address = {2455 Teller Road,~Thousand Oaks~California~91320~United States of America},
  doi = {10.4135/9781412983655},
  urldate = {2025-06-03},
  isbn = {978-0-7619-2804-1 978-1-4129-8365-5}
}

@article{zijlmansItemScoreReliabilitySelection2019,
  title = {Item-{{Score Reliability}} as a {{Selection Tool}} in {{Test Construction}}},
  author = {Zijlmans, Eva A. O. and Tijmstra, Jesper and Van Der Ark, L. Andries and Sijtsma, Klaas},
  year = {2019},
  month = jan,
  journal = {Frontiers in Psychology},
  volume = {9},
  pages = {2298},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2018.02298},
  urldate = {2025-06-03},
  file = {/Users/elinaschweppe/Zotero/storage/4USJK9EK/Zijlmans et al. - 2019 - Item-Score Reliability as a Selection Tool in Test.pdf}
}

@article{barakCognitiveAgeNonchronological1981,
  title = {Cognitive Age: {{A}} Nonchronological Age Variable.},
  author = {Barak, Benny and Schiffman, Leon G.},
  year = {1981},
  volume = {8},
  number = {1},
  issn = {0098-9258},
  journal = {Advances in Consumer Research},
  pages = {602-606}
}


@article{tenholtScaleConstructionEvaluation2010,
	title = {Scale construction and evaluation in practice: {A} review of factor analysis versus item response theory applications.},
	volume = {52},
	issn = {2190-0507(Electronic),2190-0493(Print)},
	abstract = {In scale construction and evaluation, factor analysis (FA) and item response theory (IRT) are two methods frequently used to determine whether a set of items reliably measures a latent variable. In a review of 41 published studies we examined which methodology - FA or IRT - was used, and what researchers' motivations were for applying either method. Characteristics of the studies were compared to gain more insight into the practice of scale analysis. Findings indicate that FA is applied far more often than IRT. Many times it is unclear whether the data justify the chosen method because model assumptions are neglected. We recommended that researchers (a) use substantive knowledge about the items to their advantage by more frequently employing confirmatory techniques, as well as adding item content and interpretability of factors to the criteria in model evaluation; and (b) investigate model assumptions and report corresponding findings. To this end, we recommend more collaboration between substantive researchers and statisticians/psychometricians. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
	number = {3},
	journal = {Psychological Test and Assessment Modeling},
	author = {ten Holt, Janke C. and van Duijn, Marijtje A. J. and Boomsma, Anne},
	year = {2010},
	note = {Place: Germany
Publisher: Pabst Science Publishers},
	pages = {272--297},
}


@book{prueferVerfahrenEvaluationSurvey1996,
title = {Verfahren zur Evaluation von Survey-Fragen: ein Überblick},
author = {Prüfer, Peter and Rexroth, Margrit},
year = {1996},
series = {ZUMA-Arbeitsbericht},
pages = {34},
volume = {1996/05},
address = {Mannheim},
publisher = {Zentrum für Umfragen, Methoden und Analysen -ZUMA-},
urn = {https://nbn-resolving.org/urn:nbn:de:0168-ssoar-200204},
abstract = {Wer Daten mittels Umfragen erhebt, kennt das Problem: Werden die Fragen des Fragebogens 'gute' Daten liefern, d. h. werden sie zuverlässig das messen, was sie messen sollen und damit reliable und valide Antworten liefern? Heute stehen zur Evaluation von Fragen eine ganze Reihe von Verfahren zur Verfügung. Der vorliegende Beitrag rekapituliert einige der neueren Entwicklungen in diesem Bereich. Die sozialwissenschaftliche Methodenforschung, die im Bereich der Fragebogenkonstruktion durch die Zusammenarbeit mit Kognitionsforschern in den letzten Jahren zu äußerst praxisrelevanten Erkenntnissen kam, bezog seit Mitte der 80er Jahre auch den Pretestbereich mit ein. Diese neuen kognitionspsychologischen Verfahren bieten den Vorteil, Einblick in die Gedankenprozesse der Befragten zu gewinnen, um so Probleme bei Fragen zu identifizieren. Im Gegensatz dazu ist die Identifizierung vom Problemen beim Standard-Pretest ja nur dann der Fall, wenn Befragte selbst um Klärung bitten oder sich offensichtlich falsch verhalten. Insbesondere hat der Einsatz solcher Verfahren dazu beigetragen, Erkenntnisse bei der Beantwortung retrospektiver Fragen zu gewinnen. (ICE)},
keywords = {Validität; cognition; methodology; evaluation; Methodologie; survey; interview; Befragung; Verstehen; Kognition; data collection method; empirical social research; Interview; Evaluation; validity; Reliabilität; reliability; understanding; empirische Sozialforschung; Erhebungsmethode}
}


@book{prueferKognitiveInterviews2005,
title = {Kognitive Interviews},
author = {Prüfer, Peter and Rexroth, Margrit},
year = {2005},
series = {GESIS-How-to},
pages = {26},
volume = {15},
address = {Mannheim},
publisher = {Zentrum für Umfragen, Methoden und Analysen -ZUMA-},
urn = {https://nbn-resolving.org/urn:nbn:de:0168-ssoar-201470},
abstract = {'In der Umfrageforschung sind kognitive Interviews ein effektives Werkzeug, um potentielle Probleme bei Survey-Fragen zu identifizieren. In diesem Beitrag werden die wichtigsten kognitiven Techniken vorgestellt und Empfehlungen für die Durchführung kognitiver Interviews gegeben.' (Autorenreferat)},
keywords = {cognition; Interview; survey research; interview; Umfrageforschung; Kognition}
}
