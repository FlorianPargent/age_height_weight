---
title: "Item Construction Rules Revisited: Learnings from Measurement of Latent Variables with Gold Standard Items"
shorttitle: "Measuring latent variables with gold standard items"
date: today
date-modified: today
date-format: iso
author:
  - name: Kathryn Eichhorn
    equal-contributor: true
    orcid: 0000-0003-3676-9420
    role: 
      - conceptualization
      - investigation
      - resources
      - data curation
      - writing
    affiliations:
      - name: University of the Bundeswehr Munich
        department: Institute of Psychology
        city: Neubiberg
        country: Germany
  - name: Markus BÃ¼hner
    corresponding: true
    equal-contributor: true
    email: buehner@lmu.de
    orcid: 0000-0002-0597-8708
    role: 
      - formal analysis
      - supervision
      - writing
    affiliations:
      - id: "lmu"
        name: LMU Munich
        department: Department of Psychology
        address: Leopoldstr. 13
        city: D-80802 Munich
        country: Germany
        postal-code: 80802
  - name: Florian Pargent
    orcid: 0000-0002-2388-553X
    role:
      - formal analysis
      - software
      - validation
      - editing
    affiliations:
      - ref: lmu
  - name: Janika Saretzki
    orcid: ~
    role:
      - editing
    affiliations:
      - name: University of Graz
        department: Department of Psychology
        city: Graz
        country: Austria
      - name: Charlotte Fresenius University of Psychology
        city: Munich
        country: Germany
      - ref: lmu
  - name: Larissa Sust
    orcid: 0000-0002-3389-1626
    role:
      - editing
    affiliations:
      - ref: lmu
  - name: Jonas Hauck
    orcid: ~
    role:
      - editing
    affiliations:
      - id: "regen"
        name: University of Regensburg
        department: Faculty of Psychology
        city: Regensburg
        country: Germany
  - name: Sven Hilbert
    orcid: ~
    role:
      - conceptualization
      - supervision
      - editing
    affiliations:
      - ref: regen
    
author-note:
  disclosures:
    data-sharing: This is version 1 of our preprint published at <LINK>. All materials (reproducible manuscript, analysis code, data) are available in the project's repository on the Open Science Framework (OSF) at <LINK>. A Quarto Manuscripts website is hosted at <LINK>.
    conflict-of-interest: The authors declare that there were no conflicts of interest concerning the authorship or the publication of this article.
    financial-support: ~
    gratitude: ~
    authorship-agreements: ~
language:
  title-block-role-introduction: "Author roles were classified using the Contributor Role Taxonomy (CRediT; <https://credit.niso.org/>):"
abstract: This article presents four studies (one qualitative study with n = 8, three quantitative studies with n = 390, n = 921, n = 931) to investigate whether gold-standard items are helpful for questionnaire construction. We constructed three 12-item scales to measure the physical traits of body height, body weight, and age, based on a 2- and a 6-response category response format. In addition, we assessed numeric self-reports of body weight, body height, and age as gold-standard items. Confirmatory factor analyses revealed that the gold-standard items did not always exhibit the highest loading on their latent variable. Furthermore, when controlling for the self-reported physical body height, body weight, and age as gold-standard items and gender, an interpretable, systematic residual variance did not always remain. Finally, the pattern of correlations between the latent variables did not reflect the correlations between the self-reported gold standard items indicating that they do not have the same validity. While these results are in line with existing literature, our analyses also showed that items with two response categories were at least as valid as those with six categories, which contradicts past recommendations. When constructing the questionnaire, the items that are intended to measure the latent variable itself should have the highest loading on the latent variable. If this is not the case, the content validity is at least questionable. Implications are that intensive cognitive pretesting is necessary. Questionnaires with different response formats should be empirically compared and most important, hypotheses about items which should represent the latent variable best should be tested.
keywords: [item construction, latent variable modeling, physical scales, gold standard items, response categories]
license: CC BY 4.0
bibliography: [references.bib]
floatsintext: true
numbered-lines: false
csl: apa.csl
---

```{r}
#| label: setup
#| include: false
library(foreign)
library(flextable)
dat_poly <- read.spss("data/Haupterhebung Studie III (Polytom 6PS).sav", 
  to.data.frame = TRUE, use.value.labels = FALSE, na.omit = FALSE)
dat_dich <- read.spss("data/Haupterhebung Studie III (Dichotom 2PS).sav",
  to.data.frame = TRUE, use.value.labels = FALSE, na.omit = FALSE)

results <- readRDS("results/results.rds")
results_secondary <- readRDS("results/results_secondary.rds")
```

# Introduction

In psychological research, studies typically develop questionnaires and use latent variable ap-proaches to test whether they measure the intended latent variables [@tenholtScaleConstructionEvaluation2010]. According to the most applied test models (e.g., factor analytic models) especially item discrimination parameters are interpreted as how well items correlate with the latent variable [see @lordStatisticalTheoriesMental1968]. Highly correlated items with the latent variable indicate that these items are more important to measure the latent variable compared to items with lower correlations with the latent variable. Thus, the item discrimination parameter is typically used to assess the quality of an item measuring the latent variable.

To assure that we obtain high item discrimination parameters it is essential to start with a precise definition of the latent variable. The definition is the starting point for deriving content valid items. The item construction is believed to be theoretically sound if items are selected from a larger population of possible items of the latent variable in a representative manner to achieve high content validity. Applying this procedure, we have no doubt that we measure the indented latent variable. Let us assume we try to measure the latent variable warmth with the following adjectives: *affectionate*, *friendly*, *talkative*, *unprejudiced*. We ask how well each adjective describe yourself from *not at all* (0) to *fully and entirely* (4) with five response categories. If these items measure warmth, they should at least all significantly correlate with the adjective *warmth* measured with the same response format. Even more, the warmth adjective should have the highest discrimination parameter conducting a factor analysis with empirical data, otherwise a different construct is measured, e.g., friendliness, in the case that the adjective *friendly* has the highest loading on the latent variable. This can be seen as kind of prove that the intended latent variable has the intended meaning. We name such items in the following sections as *gold standard items* helping to assure that we measure the intended latent variable.

In this study, we constructed scales for physical "traits" like body height, body weight, and age and compared them against a gold standard item, that is, the numerical self-report of the respective physical parameter. We derived four assumptions that should hold if a valid measurement of the mentioned latent variables is possible. 

## Theoretical Background

In the following sections we provide short sections about the relevant literature to previous studies, item wording, response categories, and the resulting implications for the present study.

### Evidence Using Gold Standard Items

To demonstrate the validity of psychological measurements and, thus, the practical utility of psychological measurement models, @bortolottiRelevanceAdvantagesUsing2013 compared psychological measurement with physical measurement. They constructed a scale with 27 items intended to assess body height. Using a response format with two response categories, participants were asked to respond to items such as "Do I think, I would do well in a basketball team?" [@bortolottiRelevanceAdvantagesUsing2013, p. 2350]. At the end of the scale, participants were asked to report their actual height in centimeters, the self-reported physical gold standard item. The parameters estimated from the latent variable body height measurement were then compared with the physical, real measurements. The results revealed a strong correlation (.86, not corrected for disattenuation) between the estimated person parameters derived from a two-parameter logistic model and the self-reported physical height. Still, the gold standard did not exhibit the highest loading on the scale [@bortolottiRelevanceAdvantagesUsing2013]. Van der Linden (2016a) also employed a height measurement based on items with two response categories (e.g., "I bump my head quite often."; van der Linden, 2016a, p. 27) to illustrate the usefulness of logistic item response models (for an overview of item response models, see van der Linden, 2016b). According to the author, this approach is unusual, as body height appears to be a physical variable that can only be measured using a yardstick. However, differences in height influence behavior, making it quite plausible to view such behavioral indicators as proxies for body height and to derive a psychological measurement from them. First, gold standard items do not have the highest loadings so far on the latent variable. Second, measuring a "physical" latent variable might be a psychological representation of body height rather than a measure of physical body height.

### Item Wording

One important aspect of item construction is the item's wording. In this context, @pargentCanMakeIt2019 showed that the quality of item wording did, in fact, not impact model fit based on commonly used fit indices of confirmatory factor analyses. Sound item wording may not to be crucial for achieving good model fit, but it clearly is for understanding the item correctly: For example, if a person does not correctly recognize an item with a negative polarity, the person's value on the scale is immediately over- or underestimated by a few points. In some cases, this can lead to considerable distortions in the test score. If a person does not understand the items correctly, their response cannot be valid, regardless of any model fit indicators. Thus, it would be problematic to evaluate the quality of a questionnaire by solely looking at the model fit of the latent variable model, the discrimination parameters and/or the reliability estimates. In this context, @mcneishThornyRelationMeasurement2018 stated that good fit indices [e.g., Root Mean Square Error of Approximation (RMSEA), Standardized Root Mean Square Residual (SRMR), and the Comparative Fit Index (CFI)] must be considered together with measurement quality (reliability) and the size of standardized the factor loadings. They showed examples where all fit indices were excellent, but the standardized item loadings were .40, and another model where the fit indices were inadequate, but the factor loadings were .90. Thus, fit indices can be misleading without considering the quality of measurement.

### Response Scale

@hilbertInfluenceResponseFormat2016 present initial evidence that different response scales might not measure the same latent variable. They found that the correlation (disattenuated for measurement error) between latent variables measured by identical items ranged only from .76 to .88 when using items with two versus five response categories and a visual analogue scale. The question arises: Which of these questionnaires measures the true construct? What is well known is how the number of response categories affects reliability. A vast amount of literature compares different numbers of response categories for rating scale items. We quote some exemplary studies to demonstrate how complex and difficult it is to choose a reasonable number of response categories and what we conclude from these studies. @prestonOptimalNumberResponse2000 reported that two response categories exhibited the lowest retest-reliability and a relatively poor validity and discriminating power. They recommend applying rating scales with at least seven response categories. In contrast, @leeSearchOptimalNumber2014 [p. 663] concluded that "caution should be made if a scale has only two response categories, but that limitation may be overcome by manipulating other scale features, namely, scale length or item discrimination." @weijtersEffectRatingScale2010 recommend rating scales with five fully labeled response categories for samples drawn from the general population. In their recommendations, the authors considered an extreme response style, responses to reversed items, and an acquiescence response style. However, @masudaRespondentsLowMotivation2017 found that low-motivated persons tend to use the middle category, implying that a middle category is not advisable. Similarly, @bradleyRatingScalesSurvey2015 [p. 8] concluded from their study that "for constructing measures from survey responses, the inclusion of a neutral middle category distorts the data to the point where it is not possible to construct meaningful measures." All in all, the reviewed literature suggests that to achieve high reliability, seven response categories seem the be useful, but there is also evidence to omit the middle category due to the inconsistent use of it. Thus, we infer that there is evidence to use rating scales with six response categories without a middle category.

### Implications

We quote from a review on the application of Item Response Theory (IRT) models and factor analyses [@tenholtScaleConstructionEvaluation2010, p. 288]: "In our opinion, researchers could take far better advantage of their theoretical knowledge and/or expectations by incorporating their a priori knowledge of the items and scales in the analyses. This should be reflected (a) by more frequent application of confirmatory techniques, especially in the construction of new scales and (b) by adding interpretability of factors and content of items to the criteria used for model evaluation." Building on this summary statement and the aforementioned research and recommendations, we assess physical "traits" to replicate and extend previous findings. First, we constructed scales for body height, body weight, and age. Second, we used items with two response categories to replicate the results from @bortolottiRelevanceAdvantagesUsing2013 for body height. Second, we applied an additional format containing six response categories without a middle category since low-motivated persons tend to use the middle [e.g., see @masudaRespondentsLowMotivation2017]. We aimed to investigate whether a) the results by @bortolottiRelevanceAdvantagesUsing2013 hold true for body height and for other constructs as well, and whether b) there really is a psychological component as assumed by van der Linden (2016a). Furthermore, we assumed that items with six response categories have higher reliabilities and thus, higher loadings and better validities than those with two response categories [@leeSearchOptimalNumber2014; @prestonOptimalNumberResponse2000; @weijtersEffectRatingScale2010]. The concrete assumptions derived from this procedure were the following:

-	Assumption 1: From a theoretical, not an empirical, point of view, the self-reported physical body height, body weight, and age as the self-reported gold standard items have its highest loadings on the respective factors (which is equivalent to a high correlation between the latent variable and the item applying a one-factor model).

-	Assumption 2: The measurement has a psychological component, since the self-reported physical measurement and gender [as there are gender differences in perception, see @mccrearyGenderAgeDifferences2002] cannot fully explain the item responses. Systematic correlated residuals remain.

-	Assumption 3a, b:  The correlational pattern between the latent variables for the construct corresponds to the correlation of the self-reported gold standard items (3a) and the factor loadings and reliabilities are higher for 6-point response categories compared to the 2-point response categories (3b).

# Methods

## Item Construction

First, we constructed items for the physical characteristics of body height, body weight, and age. Age was defined as chronological age or age in years, corresponding to the time elapsed since a person's birth (Montepare & Lachman, 1989; Schwall, 2012). Height was defined as the height of an upright person from the sole of the foot to the top of the head in centimeters, and body weight as the physical mass of a person in kilograms (Martin, 1929). The items were developed by 124 psychology students (deductive item construction, Burisch, 1984) and 24 persons (prototype approach, see Broughton, 1984) who deviate from the German population mean at least one standard deviation in the relevant characteristics (separately for women and men). The 24 persons were asked to think of prototypical behaviors for each construct to assess the whole spectrum of the latent variable. This process produced a total of 138 items (response categories were not tested). In a second step, we examined how these items were interpreted and whether they were connected to ideas unrelated to the constructs using cognitive interviews. We conducted eight interviews with a length of two to three hours and applied various cognitive procedures, including probing, paraphrasing, concurrent-think-aloud, and retrospective-think-aloud (PrÃ¼fer & Rexroth, 1996; PrÃ¼fer & Rexroth, 2005). This should be a sufficient number as recommendations suggest five to 30 interviews and indicate that the most serious problems can already be identified with a small number of interviews (Willis, 2005). The interviews were conducted with people from the target group (four women and four men) aged between 21 and 77 (*M* = 44.00, *SD* = 20.80). The level of education ranged from high school diploma to university degree. After the cognitive interviews, misleading items were reformulated or eliminated (Faulbaum et al., 2009), resulting in 61 remaining items. In a third step, these 61 items were tested online on a new sample, which initially consisted of 456 people. However, we excluded 66 subjects because they had not completed the questionnaire, resulting in a sample of 390 participants aged between 18 and 77 years (*M* = 32.00, *SD* = 12.80), including 302 women (77.4 %) and 88 men (22.6 %). The level of education was distributed as follows: 0.0 % no school leaving certificate, 3.3 % secondary school leaving certificate/elementary school or equivalent, 17.2 % secondary school or equivalent, 35.9 % vocational baccalaureate or high school diploma, 40.8 % college degree or university degree, 2.8 % doctorate or habilitation. Out of all respondents, 95.4 % stated German as their native language. Based on these survey responses, we excluded items if a) their main item loading was not on the intended factor, b) their main and secondary loadings were almost equal, or c) they had loading below or equal to .30. Additionally, if (d) items had very similar content, items with lower loadings were excluded. Based on these criteria 12 items were selected for each construct (see @tbl-5). For each of the three questionnaires, two versions were created â one with a 6-point scale with verbal endpoints ("does not apply" to "applies", only endpoints were labeled) and fully numerical anchoring for the dichotomous response format with "applies" or "does not apply".

## Statistical Analysis

All analyses were conducted in R (version 4.4.3, R Core Team, 2025). The data were analyzed using confirmatory factor analyses using the package *lavaan* (version 0.6-19, Rosseel, 2012). We used weighted least squares mean and variance adjusted (WLSMV) for parameter estimation, whereby the items were treated as ordinal and the self-reported physical height, weight, and age as continuous variables. We specified (1) a unidimensional model without any error correlations, (2) a model with one latent variable and correlated errors. In both models the self-reported physical gold standard-items and the other 12 items were specified as indicators of one latent variable. Finally, to test whether there is a latent variable while controlling for the self-reported physical item and gender, (3) a model with one latent variable where the items were additionally predicted by the manifest items self-reported physical value and gender was specified. Reliability analyses were conducted using the package *MBESS* (version 4.9.3, Kelly, 2023) and omega as internal-consistency reliability estimate is presented.

## Sample

The total sample consisted of 1,854 participants aged between 18 and 97 years (*M* = 33.00, *SD* = 16.00), including 1,036 women (55.9 %) and 818 men (44.1 %). For sociodemographic see @tbl-1. The overall sample consisted of two independent subsamples, which were divided into the 2-point and 6-point response format conditions.

@tbl-1 shows the distribution of gender, level of education, native language, and age within the two conditions. The subsamples are largely homogeneous. Due to the left-skewed age distribution and outlier values, the median is reported as a measure of the central tendency instead of the mean. The subjects were recruited in trains, colleges, universities, adult education centers, fitness studios and various public places in Germany and Austria. Participation was voluntary and without reimbursement.

```{r}
#| label: "tbl-1"
#| tbl-cap: Sample characteristics within the response category conditions
#| ft.align: left
#| disable-apaquarto-processing: false
#| apa-note: Percent = Percentage within the subsample.

results$table1
```


# Results

## Assumption 1: The self-reported physical body height, weight, and age have the highest loadings on their latent variables.

```{r}
#| label: "tbl-2"
#| tbl-cap: Standardized item loadings (correlations with the latent variable) for two and six response categories 
#| ft.align: left
#| disable-apaquarto-processing: false
#| apa-note: CFI = Comparative Fit Index (scaled). RMSEA = Root Mean Squared Error of Approximation (scaled). SRMR = Standardized Root Mean Residual. Scaled chi-square, scaled df, and scaled p-value are reported. Correlations with the latent variable higher as the self-reported physical item are printed in bold typeface as well as the best fit indices. The standardized item loadings for the self-reported physical items are presented in grey shaded boxes.

results$table2
```
::: {.content-visible when-format="html"}
*Note.* Add table note for html format here.
:::

@tbl-2 exhibits the measurement models for the three scales for body height, body weight, and age for two and six response categories each.

Both scale lengths exhibited similar loading patterns, but those with two response categories obtained slightly higher loadings compared to the six response categories in most cases. On average, questionnaires with two response categories showed better descriptive fit indices than those with six. Model fits were within the expected range for questionnaire items (Goretzko et al., 2024). In particular, the fit indices for the questionnaires on body height and weight with two response categories showed the best fit.

For the body height scales, the self-reported physical height loaded highest (six response categories) or similarly high (two response categories) on the latent variables compared to the remaining items. However, that was not the case for the weight and age scales.

For weight, the items 7 ("My pants have a small waistband.") and 4 ("I am obese.") had highest loadings on the latent variable for the questionnaire with six response categories and the items 5 ("I weigh a lot.") on the latent variable for the questionnaire with two response categories scale. Here, 50.0% of items loaded higher than the gold-standard physical weight report for both scales.

For age, the items 5 ("I've already lived most of my life.") and 7 ("Over time, my memory has deteriorated.") had the highest loadings on the latent variable for the questionnaire with six response categories and the items 10 ("My whole life still lies ahead of me.") and 5 for the questionnaire with two response categories. Overall, 30.0% (six categories) respectively. 40.0% (two categories) of the items loaded higher than the gold-standard physical age report.

The items loadings for both weight and age scales suggest that the gold-standard items were not essential for measuring the latent variable. 

Even after allowing for correlated errors as suggested by the modification indices, the loading patterns did not improve (see notes in @tbl-5). Loading patterns with and without modifications correlated above .98 across all questionnaires (body height, body weight, age) and response variants (two and six categories; *p* < .001). Therefore, no correlated errors were specified.

## Assumption 2: The measurement has a psychological component, and the item responses cannot be fully explained by the self-reported physical measurement and gender. A systemic residual remains when corrected for self-reported physical height and gender.

For body height, we obtained overall lower item loadings on the latent variable â i.e., below .41 â but high correlations with the gold-standard physical body height item. In particular, the third item "Chairs and tables are usually too low for me." exhibited the highest loadings on the latent variable (two response categories: .405, and six response categories: .408). In contrast, item 11 "I have to stand at the front of group photos so that I can be seen clearly." and item 12 "When I hug other people in greeting, I have to bend downwards." were explained best by the self-reported physical body height independently of the response format. This pattern does not convincingly confirm the existence of a psychological latent variable after controlling for the self-reported physical measure and gender.

For body weight, the items with the highest loadings on the latent variable also exhibited high associations with the self-reported physical body weight (e.g., item 4 "I am obese."). Item 5 "I weigh a lot.", and item 6 "I need to lose weight." show the highest correlation were explained best by the self-reported physical body weight. Item 9 "I have a lot of space between the armrests in airplane seats." had a low loading on the latent variable but was well explained by the self-reported physical body weight. Notably, item 11 "If I have the choice between the elevator and the stairs, I take the elevator." was not related to either the self-reported physical item or the latent variable, indicating that it may represent a more health-related perspective. Taken together, the resulting latent variable is hardly interpretable. The results for the two and six response categories show the same pattern.

```{r}
#| label: "tbl-3"
#| tbl-cap: Standardized item loadings on the latent variable for two and six categories controlling for the self-reported physical item and self-reported gender
#| ft.align: left
#| disable-apaquarto-processing: false
#| apa-note: Standardized regression weights of the self-reported physical item and self-reported gender greater than .60 are printed in bold typeface. LV = TODO

results$table3
```
::: {.content-visible when-format="html"}
*Note.* Add table note for html format here.
:::

As we can see in @tbl-3, not all questionnaires show systematically correlated residuals. For age, two items did not load on the latent variable after controlling for physical age and gender. Instead, the two items (item 3 "I have a lot of life experience.", and item 9 "I have years of work experience.") showed a high association with self-reported physical age. In contrast, the three items most prototypical of the latent variable were item 2 "Over time, my mental capacity has decreased.", item 5 "Over time, my memory has deteriorated.", and item 4 "Over time, my ability to react has decreased.". Thus, the latent variable might be interpreted as mental age, but not all items have loading on the general factor. These patterns were highly similar for both the two- and the six-category response scales.

When looking at self-reported gender, there are some items with moderate standardized regression weights predicting body height [e.g., item 8 ("Pants are often so short for me.")] and body weight [e.g., item 6 ("I need to lose weight.")]. These items may cause problems when applied to men and women (differential item functioning; see Hilbert et al., 2022, for a comparable case). Finally, there are only non-significant low standardized regression weights of self-reported gender predicting age items.

## Assumption 3a, b

It is assumed that the correlational pattern between the latent variables for the construct corresponds to correlation of the self-reported gold standard items and the validity is higher for the 6-point response categories compared to the 2-point response categories.

```{r}
#| label: "tbl-4"
#| tbl-cap: Latent correlations between the latent variables age, body height and body weight, reliability estimates, and differences in $r^2$ and the corresponding self-reported physical items for 6 and 2 categories
#| ft.align: left
#| apa-note: TODO.

results$table4
```
::: {.content-visible when-format="html"}
*Note.* Above the diagonal the correlations between the latent variables are shown (printed in bold typeface) and below the diagonal the correlations between the self-reported physical measures are shown. The latent correlations were taken out of latent variable models (two and six response categories) including all self-reported physical items and all latent variables with the allocated items. The models showed the following fit: scaled chi-square (six response categories): 6203.19, df = 696, p < .001, scaled CFI: .858, scaled RMSEA = .092, SRMR= 0.109; scaled chi-square (two response categories): 3440.02, df = 696, p < .001, scaled CFI: .851, scaled RMSEA =. 065, SRMR = .122. In the diagonal (grey shaded boxes) the reliability estimates Omega for the scales can be found. The differences between the correlations of the latent variables and the self-reported physical items are shown in parentheses. * p $\leq$ .05. 
:::

As depicted in @tbl-4, the correlations between the latent variables do not match those between the self-reported physical items. While height and age obtained relatively similar correlations, the correlations of weight and age were overestimated by the latent variables and those between height and weight were underestimated (difference in $r^2$ > .06) by the latent variables (difference in $r^2$ > .30). Most notably, correlations did not differ substantially between the questionnaires with two and with six response categories. Descriptively, there seems to be a slight advantage for the two response categories. However, reliability estimates (Omega) were lower for this condition compared to the six response categories.


# Discussion


## Summary

The aim of this work was to find out whether gold standard items have the highest loadings on the corresponding latent variable. This was investigated using items with two and six response categories which measure these physical latent variables. The results reveal that this is only an exception, and two response categories perform not necessarily worse than six response categories regarding validity and reliability. Gold standard items might help to ensure the interpretation of the latent variable. The choice of the response format should be based on empirical testing and extensive cognitive pretesting. It seems not sufficient to take advice from choosing the number of response categories the literature to optimize reliability. The benefit of gold standard items und this will be discussed in the following sections.

## Gold Standard Items

Gold-standard items did not work as theoretically but empirically expected in this study. However, we still believe they are important. Only the gold standard item for body height â for six, not for two response categories â had its expected highest loading on the latent variable. Bortolotti et al. (2013), who previously applied the same procedure with two response categories to investigate a gold-standard item for body height, found a correlation of .86 with body height. This correlation pretty much matches our correlation with the latent variable. For our 2-response category scale, the correlation was slightly higher (.87), but there were also items which had descriptively equal or higher correlations (biserial correlations) with the sum score than the self-reported physical item (Pearson correlation). This can be due since we reported a latent correlation corrected for diattenuation, and our scale had 12 instead of 27 items. Nevertheless, our results depict a conceptual replication of Bortolotti et al. (2013) with a different questionnaire.

The results differed for the other latent variables, which exhibited lower correlations between the latent variable and their gold standard items. This seems problematic as these items were designed to be the "gold" measure of the latent variable and should ensure the interpretation. Thus, we should carefully consider what this finding means for item construction. If this basic requirement is not met, even for constructs that are so obvious and easy to measure, what about more complex constructs? To avoid a misleading interpretation of a latent variable, it might be very helpful to formulate a priori hypotheses about the importance of the constructed items as we do it to test the construct validity of a test. If we, for example, want to measure *warmth* as a big five facet and construct items that circumscribe this facet, then an item that directly measures warmth should have its highest loading on the latent variable, otherwise its interpretation is not justified.

We must be aware, that the model fit does not prove how well the items are formulated (see Pargent et al., 2019). The model fit does also not reveal if the items are helpful to measure the latent variable. The model fit simply compares a model implied variance/covariance matrix with an empirical variance/covariance matrix. In this study, the a priori most suitable items did not have always the highest discrimination parameters or loadings. Thus, simply selecting items according to their loadings and leaving the field to the factor analysis to decide which item is good or not is likely not enough. There must be an item content loading fit.

The reliability estimate of the questionnaire informs us only about the precision of measurement and not whether our measurement is valid. This does not necessarily contradict attempts to choose items based on reliability (e.g., see Zijlmans et al. 2019) since items must have high loadings to achieve high reliability estimates of a scale.

Thus, we suggest using gold standard items as described above. It should be noted that it is critical to implement the gold standard item in the final scale since we then create a part-whole relation. This no desirable situation since all test models (classical an IRT models) assume that the probability of a positive response of a person to an item should not depend on this personâs response to any other item (Debelak & Kollar, 2020).

## Physiological Constructs as Psychological Variables

Previous hints suggest that measuring physical attributes through psychological questionnaires is not always connected to a psychological latent variable guiding behavior (Hilbert et al., 2022; van der Linden, 2016a, b). Similarly, in our study, it remained unclear if a psychological latent variable beyond body height and weight was measured and how such a latent variable could be interpreted. Only for age there was a latent variable, which can be interpreted as cognitive or psychological age (Barak & Schiffman, 1981). Furthermore, it seems helpful to include additional items, that are not intended to be measured but may further explain the item responses of interest, such as gender or items from the nomological net to reveal possible dependencies and to decide whether these dependencies are in line with the definition of the latent variables.

## Number of Categories

As expected, reliabilities were higher for scales with six response categories compared to those with two. The results regarding to the model fit as well as to validity, indicated as correlations between latent variables compared to the correlation between gold standard items, are mixed. Some of the fit indices turned out to be descriptively better for items with two response categories and some for six. The same holds true for the differences between the correlations of the gold standard items and the correlations of the latent variables. Except for reliability, the study revealed no clear picture to choose two or six response categories. Hilbert et al. (2016, 2022) showed that the same items with a different number of response categories measure not the same latent variable. This result highlights that the choice of the number of response categories constructing a questionnaire should be based on an empirical study comparing several options accompanied by an intensive cognitive pretesting and not only based on the literature which focuses on maximal reliability. 

## Recommendations

The results presented here prompt reflection and reconsideration of item construction. First, it may be helpful to consider content validity when developing items. Items that either represent a kind of gold standard with specific hypotheses about the ranking of the loadings or construct divergent items that (in)validate the items intended to measure one latent variable should be included in the construction process. Revisiting the example of *warmth* as a personality facet, such additional validation items could, for example, measure warmth itself or openness. Openness as scale or marker items that show high correlations with these warmth items should not be included in the scale. These demands are not entirely new, but newly emphasized by our analyses, calling for more stringent item construction approaches. Second, the response scale choice may not be as clear-cut as previously suggested (e.g., Lee and Paek, 2014). We found that items with two response categories were not per se unfavorable, as often suggested in the literature. While our binary scales obtained lower reliability estimates, as expected (see Preston & Coleman, 2000; Revilla et al., 2014), other results regarding validity and model fit are mixed. Since reliability is easier to control for by adding more suitable items, construct validity seems to be the stronger argument when choosing response categories. In sum, we believe that decisions on response scales should be empirically founded, especially in high stakes situations (e.g., clinical diagnosis or personnel selection), comparing alternative numbers of response categories. In this context, it is important to recognize that the number of response categories can influence what latent variables are measured (see Hilbert et al. 2016; 2022). Thus, comparing the construct validity of questionnaires differing in the number of response categories is essential. Nevertheless, it remains necessary to subject response scales to intensive cognitive pretesting to determine a suitable number of categories. On the one hand, cognitive pretesting can reveal when single categories are systematically overlooked by respondents lacking information or cognitive capacity to differentiate categories. On the other hand, cognitive pretests help assess participant burden, which should always be kept in mind. For example, two-point scales, which are commonly criticized, have previously been shown to require less effort from the respondents (Hilbert et al, 2016). 

## Conclusion

All in all, based on the results of the present study, we advocate the inclusion of gold standard items to check the interpretation of latent variables in the construction of the questionnaire. In addition, for questionnaires that are used to make particularly important decisions, we recommend empirically testing the choice of response format and using validity in combination with reliability to make a well-founded decision for or against a response format. The pre-selection of response formats should be based on cognitive pretests of the items. 



# References

::: {#refs}
:::

# CFA models with correlated errors {#apx-1}

```{r}
#| label: "tbl-5"
#| tbl-cap: Latent variable loadings for two and six response categories with correlated errors
#| ft.align: left
#| disable-apaquarto-processing: false
#| apa-note: TODO.

results_secondary$table5
```
::: {.content-visible when-format="html"}
*Note.* CFI = Comparative Fit Index (scaled). RMSEA = Root Mean Squared Error of Approximation (scaled). SRMR = Standardized Root Mean Residual. Scaled chi-square, scaled df (degrees of freedom), and scaled p-value are reported. Best Fit indices are printed in bold typeface. Loadings higher than the loading of the self-reported physical item are printed in bold typeface. The following correlated errors were specified according to high modifications indices and good interpretability (In parenthesis error correlations for two and six response categories): "I have to look up when I talk to other people." (H10) and "I have to stand at the front of group photos so that I can be seen clearly." (.568, .443); "In the supermarket, I can reach the things on the top shelf without any problems." (H4) and "I need a chair if I want to get things from the top shelf." (H9) (-.506, -.397); "I need to lose weight." (W6) and "I should eat less." (W12) (.706, .550); "I have a wide waistband when it comes to pants." (W3) and "My pants have a small waistband." (W7) (-.552, -.331); "Over time, my mental capacity has decreased." (A3) and "I have years of work experience." (A9) (.534, .492); "Over time, my mental capacity has decreased." (A2) and "Over time, my memory has deteriorated." (A5) (.625, .405); "I have already lived most of my life." (A7) and "My whole life still lies ahead of me." (A10) (-.411, -0.476).
:::

# Item translations {#apx-2}

```{r}
#| label: "tbl-6"
#| tbl-cap: Translated items used in this study
#| ft.align: left
#| disable-apaquarto-processing: false
#| apa-note: German wording in parenthesis.

knitr::kable(head(cars))
```
::: {.content-visible when-format="html"}
*Note.* Add table note for html format here.
:::
